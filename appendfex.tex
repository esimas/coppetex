\chapter{Aspectos Teóricos das Técnicas de Extração de Características}
\label{apend_fex}

Neste apêndice serão fornecidos os detalhes da teoria envolvida nos
diversos métodos de extração de características utilizados neste
trabalho.

\section{Mapas auto-organizáveis}

\subsection{Quantização vetorial por aprendizado}

A quantização vetorial (VQ - \textit{Vector Quantization}) é uma
técnica de codificação onde um espaço de entrada é mapeado em um
grupo finito de vetores repre\-sentativos (\textit{codebook})
\cite{article:gersho:1982}. A codificação é definida como um
particionamento do espaço de entrada em um número finito de regiões.
O quantizador realiza um mapeamento do espaço $\mathbb{R}^k$, em um
subconjunto finito $Y$ de $\mathbb{R}^k$:

\begin{equation}\label{lv1}
\begin{array}{cc}
  Q: & \mathbb{R}^k \rightarrow \mathbf{Y}
\end{array}
\end{equation}
\\
sendo $ \mathbf{Y}=\{y_1,y_2,...,y_k\}$ o livro de código (\textit{codebook}). Para cada
palavra de código $y_i$ existe uma partição $R_i$ do espaço de
entrada que satisfaz:
\begin{equation}\label{lv2}
R_i=Q^{-1}(\mathbf{y_i})=\{ \mathbf{x} \in  \mathbb{R}^k :
Q(\mathbf{x})=\mathbf{y}_i \}
\end{equation}
\begin{equation}\label{lv3}
    \begin{array}{ccc}
  \bigcup_{i=1}^{N} R_i= \mathbb{R}^k, &  R_i \bigcap R_j=0,&i\neq j
\end{array}
\end{equation}
\\
Quando um quantizador vetorial possui mínima distorção é denominado
\textbf{quantizador de Voronoi}. Neste caso, diz-se que o espaço de
entrada está particionado de acordo com a regra do vizinho mais
próximo, e as partições criadas são chamadas de células de Voronoi
\cite{article:gray:1984}. Usando-se a distância euclidiana como
parâmetro de distorção, o quantizador $Q^*$ é dito ótimo se, para
qualquer outro quantizador $Q$, com o mesmo número de pontos, a
condição abaixo é satisfeita:

\begin{equation}\label{llv}
    E|| \mathbf{x} - Q^*(\mathbf{x}) ||^2 \leq E|| \mathbf{x} - Q^(\mathbf{x}) ||^2
\end{equation}
\\
As palavras de código ou os vetores de Voronoi podem ser calculados
de modo aproximado pelo algoritmo SOM. O \textit{codebook} é formado a partir
dos pesos sinápticos dos neurônios do mapa. As células de Voronoi
são compostas pelos pontos do espaço de entrada que estão mais
próximos do vetor de código correspondente.

Em um problema de classificação, pode-se empregar a quantização vetorial por aprendizado (\textit{Learning Vector
Quantization}) \cite{article:kohonenLVQ:1990}, que utiliza informações sobre as classes para mover
ligeiramente os vetores de Voronoi, visando a uma melhora no desempenho de decisão do classificador.

Na sua forma básica, o algoritmo LVQ escolhe aleatoriamente um vetor
de entrada $\mathbf{x}$; quando seu rótulo de classe
$\mathcal{C}_{\mathbf{x_i}}$ e o de um vetor de Voronoi
$\mathbf{w_c}$ concordam, então, $\mathbf{w_c}$ é movido na direção
de $\mathbf{x}$:

\begin{equation}\label{lvq1}
  \mathcal{C}_{\mathbf{w_c}}=\mathcal{C}_{\mathbf{x_i}} \rightarrow \mathbf{w_c}(n+1)=\mathbf{w_c}(n)+
    \alpha[\mathbf{x}-\mathbf{w_c}(n)]
\end{equation}
\\
onde $\alpha$ é a taxa de aprendizagem ($0< \alpha <1$). Em caso
contrário, $\mathbf{w}$ é afastado de $\mathbf{x}$:

\begin{equation}\label{lvq2}
  \mathcal{C}_{\mathbf{w_c}}\neq \mathcal{C}_{\mathbf{x_i}} \rightarrow
  \mathbf{w_c}(n+1)=\mathbf{w_c}(n)-
    \alpha[\mathbf{x}-\mathbf{w_c}(n)]
\end{equation}
\\
Conforme proposto em \cite{article:kohonenLVQ:1990}, podem ser
implementadas algumas modificações na forma básica do algoritmo de
LVQ, visando a melhorar o desempenho do método. Chega-se, então, aos
algoritmos LVQ-2 e LVQ-2.1, que ajustam dois vetores de código
simultaneamente.

Alguns exemplos da aplicação da quantização vetorial por aprendizado
para compressão de sinais e classificação podem ser encontrados em
\cite{article:kohonenLVQ:1990} e \cite{article:dey:1999}.

\subsection{Classificação a partir do mapa de
características}\label{mlp_somm}

Considerando um problema de classificação, o mapeamento
auto-organizável consegue transformar o conjunto de sinais,
revelando características ocultas. A nova organização do conjunto de
entrada pode ser utilizada para guiar o processo de discriminação.
Em \cite{book:freeman:1991} é proposta uma estratégia de
classificação a partir do mapa de características onde uma rede
neural MLP é conectada às saídas do SOM (ver Figura
\ref{som_class}). A MLP é treinada com supervisão usando informações
a respeito das classes de sinais.

\begin{figure}[tbph]
\centering
\includegraphics[width=10cm]{cap3_somclass}
\caption{Diagrama da classificação a partir do mapa de
características} \label{som_class}
\end{figure}


\section{Análise de Componentes Principais}

\subsection{Redução de dimensão}

\begin{figure}[tbph]
\centering
\includegraphics[width=5.5cm]{cap3_pca}
\caption{Compressão e recuperação do sinal $\mathbf{x}$ utilizando a
transformação por PCA.} \label{pca}
\end{figure}

A principal aplicação da PCA é a compactação da informação. A
redução de dimensão é obtida utilizando-se para a reconstrução do
sinal original $\mathbf{x}$ um número $K$ de componentes principais
sendo $K<N$. Na Figura \ref{pca} é ilustrado o processo de redução
de dimensão utilizando análise de componentes principais. Em geral,
o número de componentes é escolhido visando a preservar uma parcela
$V_e$ da energia total, de modo que $\mathbf{\widehat{x}} \approx
\mathbf{x}$. A variância explicada $V_e$ de um conjunto de
componentes pode ser calculada usando-se:
\begin{equation}\label{vexp}
    V_e(K)=\frac{\displaystyle\sum_{i=1}^K d_i}{\displaystyle\sum_{i=1}^N d_i},
\end{equation}
\\
sendo $d_i$ o autovalor da matriz $\mathbf{C}_x$ de covariância do
processo correspondente à componente $i$.

A transformação por PCA é ótima no sentido de representação do sinal
nas primeiras componentes, mas não há garantia de que a compactação
facilite o processo de classificação. Quando as direções de maior
variância coincidem com as de melhor discriminação das classes,
então a PCA é também útil para o reconhecimento de padrões, em caso
contrário, a redução de dimensão pode dificultar a separação.
Entretanto, em problemas de classificação onde a dimensão da entrada
é excessivamente grande o pré-processamento por PCA reduz o custo
computacional e conseqüentemente o tempo de processamento.


\section{Análise de Componentes Independentes}

\subsection{Princípios de estimação das componentes independentes}

No modelo básico da ICA (ver equação (\ref{icamatrix}))assume-se que
a matriz $\mathbf{A}$ é quadrada e não são considerados os atrasos
temporais nem a existência de ruído aditivo. O princípio básico para
a extração das componentes independentes é obtido do teorema do
limite central. Como a soma de duas variáveis aleatórias
independentes é sempre mais próxima de uma distribuição normal do
que as variáveis originais, os sinais misturados $x_i$, que são
gerados a partir do somatório ponderado das fontes $s_i$, têm
distribuições de probabilidade mais semelhantes à Gaussiana quando
comparadas aos sinais originais. As fontes podem ser obtidas então
pela maximização da não-Gaussianidade.
%A curtose e a negentropia são usualmente
%utilizadas para medição da não-gaussianidade em ICA.

\subsubsection{Maximização da não-Gaussianidade}

A \textbf{curtose} é o cumulante de quarta ordem, e para uma
variável $y$ de média zero e variância unitária é definida por
\cite{book:peebles:2001}:
\begin{equation}\label{curtose}
    kurt(y)=\mathcal{E}\{y^4\}-3(\mathcal{E}\{y^2\})^2.
\end{equation}

Variando no intervalo $[-2,\infty)$, a curtose é igual a zero para
uma variável Gaussiana, os valores negativos indicam
sub-Gaussianidade e os positivos super-Gaussianidade.
%Nas Figuras
%\ref{subgauss} e \ref{supergauss} pode-se visualizar exemplos dos 3
%tipo de distribuições, gaussiana ou normal, sub-gaussiana (mais
%achatada) ou super-gaussiana (mais concentrada em torno da média). O
%menor valor da curtose ocorre para variáveis uniformemente
%distribuídas.
%
%\begin{figure}[tbph]
%\begin{center}
%\subfigure[]{\label{subgauss}\epsfig{file=cap3_subgauss,width=11cm,clip=}}
%\subfigure[]{\label{supergauss}\epsfig{file=cap3_supergauss,width=11cm,clip=}}
%\end{center}
%\caption{Distribuições de probabilidade (a) sub-gaussiana e (b)
%super-gaussiana.}
%\end{figure}

A curtose é um parâmetro estatístico facilmente calculado a partir
das rea\-lizações da variável aleatória, porém seu valor pode ser
bastante influenciado por um pequeno conjunto de pontos na cauda da
distribuição \cite{book:spiegel:stat}, sendo, nesse caso, pouco
robusta para a estimativa da não-Gaussianidade. Conhecidos como
intrusos (ou \textit{outliers}) esses pontos podem realmente
pertencer à variável aleatória, ou ter sido artificialmente
introduzidos por algum fenômeno desconhecido, como erro de medida ou
de digitação.

Uma estimação alternativa da não-Gaussianidade pode ser obtida a
partir da \textbf{negentropia}, que é calculada por
\cite{book:cover:1991}:
\begin{equation}\label{negen}
    J(y)=H(y_{gauss})-H(y),
\end{equation}
\\
onde $H(.)$ é a entropia, e $y_{gauss}$ é uma variável aleatória
Gaussiana com a mesma média e variância de $y$. A entropia é um dos
conceitos básicos da teoria da informação e pode ser interpretada
como o grau de informação contido em uma variável. Para uma variável
aleatória discreta a entropia é definida como
\cite{article:shannon:1948}:
\begin{equation}\label{entro1}
    H(Y)=-\sum_i P(Y=a_i)log P(Y=a_i),
\end{equation}
\\
onde os $a_i$ são os possíveis valores da variável $Y$, e $P(Y=a_i)$
é a probabilidade de $Y$ ser igual a $a_i$.

Um resultado importante obtido a partir da teoria da informação é
que uma variável Gaussiana tem a máxima entropia entre todas as
variáveis de mesma variância. Considerando a equação (\ref{negen}),
a negentropia é sempre não negativa e zero quando a variável é
Gaussiana, servindo como uma medição da não-Gaussianidade. O grande
problema no cálculo de $J(.)$ é a necessidade de se estimar as
probabilidades da equação (\ref{entro1}). Para evitar esse cálculo,
utilizam-se aproximações da negentropia. Conforme descrito em
\cite{book:oja:2001}, existem duas aproximações mais utilizadas para
a negentropia, uma faz uso de cumulantes de ordem superior:
\begin{equation}\label{eq_neg1}
    J(Y)\approx \frac{1}{12}E\{Y^3\}^2+\frac{1}{48}kurt(Y)^2,
\end{equation}
e outra utiliza funções não-polinomiais
\cite{article:hyvarinen:1998}:
\begin{equation}\label{eq_neg2}
    J(Y)\approx [k_1 (E\{G_1(Y)\})^2 +
    k_2(E\{G_2(Y)\}-E\{G_2(\nu)\})^2],
\end{equation}
onde $\nu$ é uma variável aleatória Gaussiana de média zero e
variância unitária. As funções não-lineares recomendadas em
\cite{article:hyvarinen:1998} são $G_1(y)=y\exp(-y^2/2)$ e
$G_2(y)=|y|$ ou $G_2(y)=\exp(-y^2/2)$.

O uso de cumulantes traz de volta o problema da pouca robustez a
\textit{outliers}. É mostrado em \cite{article:hyvarinen:1998} que o
uso das funções não-polinomiais leva ao método da máxima entropia
\cite{book:oja:2001}.

\subsubsection{Minimização da informação mútua}

Um outro método de estimação de ICA, também derivado da teoria da
informação, é obtido pela minimização da informação mútua. A
informação mútua $I(.)$ entre $m$ variáveis aleatórias escalares
$y_i$ é definida como \cite{article:hyvarinen:2000}:
\begin{equation}\label{mutinf}
    I(y_1,y_2,...,y_m)=\sum_{i=1}^m H(y_i) - H(\mathbf{y})
\end{equation}

A entropia $H(y_i)$ pode ser interpretada como o comprimento de
código (ou a quantidade de informação) neces\-sário para representar a variável $y_i$. Conforme a
equação (\ref{mutinf}), a informação mútua é a diferença entre o
somatório das entropias de cada uma das $m$ variáveis $y_i$ e a
entropia do vetor aleatório $\mathbf{y}=[y_1,y_2,...y_m]$. Pode-se
provar que a codificação mais eficiente é obtida quando se utiliza o
conjunto de variáveis $\mathbf{y}$. Utilizar as variáveis
isoladamente sempre gera um maior código, menos quando as $y_i$ são
independentes, pois desta forma uma variável não carrega informação
sobre as demais, sendo a informação mútua igual a zero. Desta
forma, $I(y_1,y_2,...,y_m)$ pode ser utilizada como uma medida da
dependência entre as variáveis. A matriz $\mathbf{W}$ de
transformação inversa da ICA, conforme equação \ref{icamatrix2}, pode
ser estimada através da minimização da informação mútua dos sinais
$s_i$ recuperados.

\subsubsection{ICA através da descorrelação não linear}

A igualdade da equação:
\begin{equation}\label{indep22}
    \mathcal{E}\{g(x)h(y)\}=\mathcal{E}\{g(x)\}\mathcal{E}\{h(y)\}
\end{equation}
\\
repetida aqui para comodidade
do leitor, garante que as variáveis $x$ e $y$ são independentes quando todas
funções $g(.)$ e $h(.)$,integráveis em $x$ e $y$ são
descorrelacionadas. Portanto, a extração das ICs pode ser obtida
testando-se a correlação entre todas as funções não-lineares $g(.)$ e
$h(.)$.

Existem alguns algoritmos propostos na literatura para o problema da
decorrelação não-linear, como o \textit{Hérault-Jutten}
\cite{book:oja:2001} e o \textit{Chichocki-Unbehauen}
\cite{article:Cichocki:1996}, mas como não é possível testar a
descorrelação entre todas as funções não-lineares, escolhem-se $f(.)$
e $g(.)$ visando-se a obter boas aproximações das componentes
independentes. O algoritmo \textit{Hérault-Jutten}, por exemplo,
aconselha o uso de $f(y)=y^3$ e $g(y)=arctg(y)$, já o
\textit{Chichocki-Unbehauen} sugere uma função polinomial e a
tangente hiperbólica.

A PCA não-linear (NLPCA - \textit{Non-linear Principal Component
Analysis}) pode ser vista como uma extensão não linear da PCA, e é
capaz de encontrar projeções descorrelacionadas não-linearmente.
Enquanto o objetivo da PCA é minimizar o erro médio quadrático de
reconstrução do sinal projetando as componentes numa base
ortonormal, a NLPCA pode ser definida de modo simples através da
função-objetivo a ser minimizada:
\begin{equation}\label{nlpca}
    J(\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_n)=\mathcal{E}\{||\mathbf{x}-\sum_{i=1}^n g_i(\mathbf{w}_i^Tx)\mathbf{w}_i||^2\},
\end{equation}
\\
onde $g_1(.), g_2(.), ..., g_n(.)$ é um conjunto de funções
escalares e não-lineares, e os vetores $\mathbf{w}_i$ formam a base
do sub-espaço onde serão projetadas as entradas $\mathbf{x}$. Quando
o mínimo de $J(\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_n)$ for
encontrado, o produto $\mathbf{w}_i^Tx$ dará as componentes
principais não-lineares. Se $g_i(y)=y$ para todo $i$, então equação
(\ref{nlpca}) se reduz à função objetivo da PCA. Quando os sinais
satisfazem ao modelo da ICA, mostrado na equação (\ref{icamatrix}), a
NLPCA obtém uma aproximação das componentes independentes.


\subsection{Pré-processamento dos sinais para ICA}

Em geral, os algoritmos de extração das componentes independentes
têm seu trabalho simplificado quando os sinais são centralizados, ou
seja, têm sua média removida fazendo-se:
\begin{equation}\label{remedia}
    \mathbf{x}\leftarrow \mathbf{x}-\mathcal{E}\{\mathbf{x}\}
\end{equation}

Outra transformação importante é o branqueamento. Um vetor
$\mathbf{z}=(z_1,z_2,...,z_n)^T$ é dito branco quando os elementos
$z_i$ são descorrelacionados e têm variância unitária. O
branqueamento pode ser realizado por uma transformação linear:
\begin{equation}\label{branq}
    \mathbf{z}=\mathbf{V}\mathbf{x}
\end{equation}

O branqueamento, que é apenas a descorrelação seguida de uma
norma\-lização, pode ser realizado por uma transformação através de
PCA. Com as variáveis branqueadas a extração da ICA é facilitada,
pois os sinais já estão descorrelacionados.

Em problemas com vetores de entrada de alta dimensão, é importante
a compactação da informação através de PCA ou Análise de Relevância
para facilitar o processo de extração das componentes independentes.

\subsection{Principais algoritmos para ICA}

Diversos algoritmos vêm sendo propostos para a extração das
componentes independentes. Essas rotinas diferem basicamente no
princípio teórico no qual fundamentam a obtenção das componentes
independentes (não-Gaussianidade, informação mútua, descorrelação
não-linear, etc) e na forma fazem a otimização da função objetivo
escolhida. Os principais parâmetros para avaliação de desempenho são
o tempo de processamento (complexidade computacional) e a precisão
na extração das componentes.

Um estudo comparativo entre diversos métodos de estimação das
componentes independentes foi realizado em \cite{book:oja:2001}. O
algoritmo \textbf{FastICA}, descrito com detalhes em
\cite{book:oja:2001} e \cite{article:hyvarinen:2000}, é o que
apresenta menor custo computacional. Algoritmos que realizam
descorrelação não linear e NLPCA têm desempenho semelhante ao
FastICA em termos da precisão na obtenção da matriz $\mathbf{W}$,
porém exigem maior esforço de computação. O algoritmo \textbf{JADE}
(\textit{Joint Approximate Diagonalization of Eigen-matrices})
proposto em \cite{article:Cardoso:1993} também é muito utilizado em
ICA, mostrando bons resultados.

\subsubsection{Algoritmo FastICA}

Considerando as aproximações da negentropia mostradas nas Equações
(\ref{eq_neg1}) e (\ref{eq_neg2}), e o fato de que a minimização da
negentropia leva à independência estatística, no trabalho
\cite{article:fastica:1999} foram propostos algoritmos de ponto fixo
para ICA (chamados FastICA), que utilizam iterações semelhantes às
de Newton \cite{Book:Luenberger:1984}. Entre as vantagens deste
algoritmo pode-se citar simplicidade computacional, baixa utilização
de memória e boas características de convergência
\cite{article:hyvarinen:2000}.

A partir de algumas manipulações da equação (\ref{eq_neg2}), o
algoritmo FastICA para estimação de uma componente independente é
formulado a seguir para sinais pré-branqueados:
\begin{enumerate}
  \item Escolha um vetor de pesos inicial $\mathbf{w}$ de modo aleatório;
  \item Faça $\mathbf{w}^{+}=E\{\mathbf{x}
  g(\mathbf{w}^T\mathbf{x})\}-E\{g'(\mathbf{w}^T\mathbf{x})\}\mathbf{w}$;
  \item $\mathbf{w}=\mathbf{w}^{+}/\parallel \mathbf{w}^{+}
  \parallel$;
  \item Se o algoritmo não tiver convergido voltar para o passo 2.
\end{enumerate}

Os autores sugerem o uso de uma das funções g(.) a seguir:
\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
  g_1(x) = \text{tgh}(a_1 x), \\
  g_2(x) = x \exp(-a_2 u^2/2), \\
  g_3(x) = x^3,
\end{eqnarray}
onde $1\leq a_1 \leq 2$ e $a_2 \approx 1$. A escolha da função
não-linear pode ser guiada pelas características a seguir
\cite{article:fastica:1999}: a função $g_1(.)$ é indicada quando não
há informação a respeito da estatística das componentes
independentes, pois o algoritmo apresenta resultados satisfatórios
para qualquer tipo de distribuição; o uso de $g_2(.)$ é indicado
quando as componentes independentes são super-Gaussianas e o
$g_3(.)$ deve ser utilizada para estimar componentes sub-Gaussianas.

Para estimar mais de uma componente independente pode-se utilizar
métodos de ortogonalização deflacionária como o de Gram-Schimidt
\cite{book:oja:2001}.

\subsubsection{Algoritmo JADE}

No algoritmo JADE (\textit{Joint Approximate Diagonalization of
Eigenmatrices}), as informações estatísticas de segunda e quarta
ordem são utilizadas a partir de uma abordagem tensorial. Tensores
\cite{book:tensor:2008} são generalizações de alta-dimensão das
matrizes. O tensor cumulante de quarta ordem $\mathbf{T_4}$ é uma
``matriz" de quatro dimensões onde cada elemento é definido por
$q_{ijkl}=\text{cum}(x_i,x_j,x_k,x_l)$, os índices i,j,k e l variam
de 1 até N (onde N é o número de sinais) e
$\text{cum}(x_i,x_j,x_k,x_l)$ é o cumulante de quarta ordem:
\begin{equation}\label{cum4}
\begin{split}
cum(x_i,x_j,x_k,x_l)=E\{x_i,x_j,x_k,x_l\}
  -E\{x_i,x_j\}E\{x_k,x_l\} \\ -E\{x_i,x_k\}E\{x_j,x_l\}
  -E\{x_k,x_j\}E\{x_i,x_l\}
\end{split}
\end{equation}

Sabe-se que a diagona\-li\-zação da matriz de correlação
($\mathbf{C_y}$) produz a descorrelação entre os componentes de
$\mathbf{y}$ \cite{book:oja:2001}. Para sinais independentes, apenas
quando i=k=j=l os cumulantes de quarta-ordem são diferentes de zero.
Considerando isso, os métodos Tensoriais de ICA propõe a
diagonalização de $\mathbf{T_4}$ para alcançar a independência
estatística \cite{article:Cardoso:1993}.

Embora teoricamente simples, a utilização de métodos tensoriais de
ICA exigem uma grande quantidade de recursos computacionais para a
decomposição em auto-valores de matrizes de quarta-ordem. O
algoritmo JADE propõe um método aproximado para a diagonalização de
$\mathbf{T_4}$, se tornando mais leve computacionalmente.

Considerando que os dados satisfazem o modelo da ICA para dados pré-branqueados, pode-se escrever:
\begin{equation}\label{eq_jade0}
    \mathbf{z}=\mathbf{VAs}=\mathbf{W}^T\mathbf{s}
\end{equation}
onde $\mathbf{x=As}$ são os sinais observados, $\mathbf{V}$ é a
matriz de branqueamento e $\mathbf{W}^T=VA$ é a matriz de misturas
branqueada. Neste caso, pode-se provar (ver \cite{book:oja:2001} que
o tensor cumulante de $\mathbf{z}$ tem uma estrutura especial e suas
auto-matrizes são descritas por:
\begin{equation}\label{eq_jade01}
    \mathbf{M}=\mathbf{w_m w_m}^T
\end{equation}
onde m=1,...,N e $w_n$ são as colunas da matriz $\mathbf{W}^T$

O algoritmo JADE utiliza a transformação linear $F_{ij}$ da matriz $\mathbf{M}$ definida por:
\begin{equation}\label{jade1}
    F_{i,j}(\mathbf{M})=\sum m_{kl} \text{cum}(x_i,x_j,x_k,x_l)
\end{equation}
onde $m_{kl}$ é um elemento da matriz $\mathbf{M}$.

A decomposição em autovalores é vista como um processo de
diagonalização, então busca-se a matriz $\mathbf{W}$ que diagonaliza
$F(\mathbf{M})$ para qualquer $\mathbf{M}$ (ou seja, \linebreak
$Q=\mathbf{W}F(\mathbf{M_i})\mathbf{W}^T$ é uma matriz diagonal).

A função custo do método JADE busca a diagonalização de $\mathbf{Q}$
pela maximização da soma dos elementos de sua diagonal. As matrizes
$\mathbf{M_i}$ utilizadas são as automatrizes do tensor cumulante
dos dados, pois assim tem-se um conjunto de N matrizes que contém
toda a informação relevante a respeito dos cumulantes.

Os métodos tensoriais \cite{article:fobi:1989,article:Cardoso:1993}
foram, provavelmente, a primeira classe de algoritmos capazes de
executar a ICA de modo realmente eficiente \cite{book:oja:2001}.
Atualmente, estes métodos são mais utilizados para sinais de baixa
dimensão, pois o custo computacional aumenta rapidamente com o
número de componentes a serem estimados.



\subsubsection{Algoritmo Multiplicativo com Iteração de Newton}

Um algoritmo multiplicativo para ICA foi proposto por Akuzawa e
Murata em \cite{article:akuzawa:2001}. Usando a curtose como função
para avaliar a independência, esse algoritmo utiliza estratégia de
otimização de segunda ordem, através do método de Newton
\cite{Book:Luenberger:1984}, para estimar as componentes
independentes.

O algoritmo proposto por Akuzawa não requer pré-branqueamento,
operando diretamente sobre os dados medidos. Resultados
experimentais obtidos em
\cite{article:extakuzawa:2000,article:cuenca:2003} indicam que o
algoritmo de Akuzawa apresenta melhor desempenho que FastICA e JADE
quando os sinais estão contaminados por ruído Gaussiano.

Considerando a transformação linear $\mathbf{Y}=\mathbf{CX}$, o
objetivo do algoritmo de Akuzawa é encontrar a matriz $\mathbf{C}$
que maximiza a independência entre as componentes de $\mathbf{y}$.
Os passos a seguir são executados durante as iterações:

\begin{enumerate}
\item Escolher $C_0$ (a matriz de separação inicial) e a matriz $\Delta_0$ (N $\times$ N);
\item Calcular a iteração $C_t=\exp (\Delta_t -1)C_{t-1}$;
\item Avalie a função custo em $C_t$ usando uma expansão de segunda ordem em torno de $C_{t-1}$;
\item $\Delta_t$ é escolhido como o ponto de sela da função custo;
\item Retorne para o passo 2 até convergir.
\end{enumerate}

Mais detalhes a respeito da execução do passo 4 podem ser
encontradas em \cite{article:akuzawa:2001}. Modificações no método
de Akuzawa foram propostos em \cite{article:extakuzawa:2000} com o
objetivo de reduzir o custo computacional pela substituição das
iterações de Newton pelo método quasi-Newton
\cite{Book:Luenberger:1984}.

Comparado com FastICA e JADE, o algoritmo multiplicativo de Akuzawa
é mais lento (mesmo em sua versão modificada que utiliza o método de
otimização quasi-Newton), suas vantagens aparecem quando o nível de
ruído aumenta, neste caso o método de Akuzawa apresenta melhores
resultados.


\subsection{Extensões ao modelo básico de ICA}

A análise de componentes independentes em sua formulação básica
mostrada nas equações (\ref{icamatrix}) e (\ref{icamatrix2}) é
empregada com sucesso em uma grande variedade de aplicações. O
modelo básico de ICA não considera que os sinais podem estar
contaminados por ruído aditivo, ou que o sinal misturado seja gerado
de forma não-linear. No ambiente do calorímetro do ATLAS, o ruído
estará presente nas células absorvedoras de energia, e por
características de construção do sistema, transformações
não-lineares dos sinais poderão estar presentes.

\subsubsection{ICA para sinais ruidosos}

Sabe-se que os sinais reais no ambiente do calorímetro do ATLAS
estarão contaminados por ruído. A formulação básica da ICA, mostrada
na equação (\ref{icamatrix}), não considera a presença de ruído. Um
modelo mais realista permite a obtenção do modelo da ICA para sinais
ruidosos (\textit{Noisy ICA}) \cite{book:oja:2001}:
\begin{equation}\label{noisy_ica}
    \mathbf{x}=\mathbf{As}+\mathbf{n},
\end{equation}
\\
onde $\mathbf{n}=(n_1,n_2,...)$ é o vetor de ruído. Em geral,
considera-se que o ruído é gaussiano e independente das componentes
independentes. Assume-se também que a matriz de covariância do ruído
$\Sigma=\sigma^2 I$ é conhecida.

A equação (\ref{noisy_ica}) pode ser associada a ruído de sensores,
uma vez que o ruído $n_i$ é adicionado separadamente a cada fonte
$x_i$. O ruído de fonte, que é adicionado às componentes
independentes $s_i$ é representado pela equação a seguir:
\begin{equation}\label{noisy_ica2}
    \mathbf{x}=\mathbf{A}(\mathbf{s}+\mathbf{n})
\end{equation}

Um caso especial que pode simplificar bastante o problema de
estimação das componentes independentes em ambiente ruidoso acontece
quando existem poucas fontes de ruído. Se o número total de
componentes de ruído é menor que o de componentes independentes, o
modelo básico da ICA pode ser adaptado. Define-se o vetor
$\mathbf{\widetilde{s}}=(s_1,...,s_k,n_1,...,n_l)^T$, onde $s_i$
($i=1,...,k$) são as componentes independentes e $n_j$ ($j=1,...,l$)
são as fontes de ruído. Assumindo-se que o número de misturas é
$k+l$, então o modelo básico da ICA pode ser aplicado a
$x=A\widetilde{s}$ e usando-se um algoritmo deflacionário
consegue-se estimar as $k$ componentes menos gaussianas, que são as
próprias ICs.

Na maioria das vezes, deseja-se considerar que o ruído foi
adicionado a cada uma das misturas; assim, $k+l$ é maior que o
número de misturas, e o modelo básico da ICA não pode ser aplicado
ao vetor $\widetilde{s}$.

O processo de estimação das componentes independentes já é bastante
difícil na formulação básica, e quando o ruído é considerado o
cenário piora consideravelmente. Nos modelos das equações
(\ref{noisy_ica}) e (\ref{noisy_ica2}) pode-se verificar que as ICs
não são obtidas apenas com a inversão da matriz de mistura
$\mathbf{W}$:
\begin{equation}\label{noisy_ica3}
    \mathbf{Wx}=\mathbf{s}+\mathbf{Wn}
\end{equation}

A solução do problema da equação (\ref{noisy_ica3}) envolve duas
etapas: o uso de métodos de otimização numérica para obter uma
aproximação do ruído e, a partir de então, a estimação das
componentes independentes. Em uma abordagem mais simples e que
parece ser a mais promissora para o \textit{noisy ICA}, são
utilizadas técnicas de remoção de tendência para reduzir os efeitos
do ruído, adaptando-se os métodos básicos de ICA para o caso ruidoso
\cite{article:amri:1998}.
%
%Se a medida da não-gaussianidade for a curtose, é quase trivial a
%construção de métodos deflacionários para a \textit{noisy ICA}, pois
%a curtose é imune ao ruído gaussiano \cite{book:oja:2001}. Porém
%sabe-se que os cumulantes de alta ordem são sensíveis a pontos fora
%da distribuição principal (\textit{outliers}), dificultando o uso
%para a extração das componentes independentes.

O algoritmo FastICA pode ser adaptado para sinais contaminados por
ruído usando medidas não tendenciosas da Gaussianidade
\cite{article:hyvarinen:1998noisy}:
\begin{equation}\label{noisyfastica}
    \mathbf{w}^*=E\{\tilde{\mathbf{x}}g(\mathbf{w}^T)\tilde{\mathbf{x}}\}-(I+\tilde{\mathbf{\Sigma}})\mathbf{w}E\{g'(\mathbf{w}^T\tilde{\mathbf{x}})\},
\end{equation}
\\
sendo que a norma do novo valor $\mathbf{w}^*$ é tornada unitária
após cada iteração, e $\tilde{\mathbf{\Sigma}}$ é dado por:
\begin{equation}\label{sigg}
    \tilde{\mathbf{\Sigma}}=E\{\tilde{\mathbf{n}}\tilde{\mathbf{n}}^T\}=(\mathbf{C}-\mathbf{\Sigma})^{-1/2}\mathbf{\Sigma}(\mathbf{C}-\mathbf{\Sigma})^{-1/2}
\end{equation}
\\
onde $\mathbf{C}=E\{\mathbf{x}\mathbf{x}^T\}$ é a matriz de
covariância dos sinais ruidosos observados. A função $g(.)$ pode ser
escolhida entre as abaixo:
%\begin{equation}\label{ee}
%    \begin{array}{ccc}
%       g_1(u)=\tgh(u), & g_2(u)=u exp(-u^2/2), & g_3(u)=u^3
%     \end{array}
%\end{equation}

Outras técnicas como o ``encolhimento de código esparso"
(\textit{Sparse Code Shrinkage}) \cite{article:hyvarinen:1999noisy}
também são utilizadas para a extração das componentes independentes
para sinais ruidosos. Em casos onde o nível de ruído é muito alto,
pode-se tentar o uso de técnicas de processamento de sinais como
\textit{wavelets} ou filtragem
adaptativa~\cite{article:jutten:2002}.


\section{ICA não-linear}

Conforme mostrado no Capítulo \ref{cap_fex}, o modelo da ICA
não-linear (NLICA) apresenta uma formulação mais geral que o linear.
A seguir será mostrado o desenvolvimento teórico de um algoritmo
para a estimação das componentes independentes no modelo pós
não-linear.

\subsection{Algoritmo Taleb-Jutten para o modelo PNL}

Um dos primeiros algoritmos para o modelo pós não-linear da ICA foi
proposto por Taleb e Jutten no trabalho \cite{article:jutten:1999}.
Este algoritmo é robusto a variações na distribuição de
probabilidade das fontes, pois executa estimação iterativa da
estatística das componentes independentes estimadas através do
cálculo da função escore:
\begin{equation}\label{juttenpnl1}
    \psi = p'_{Yi}(u) / p_{Yi}(u),
\end{equation}
conforme Figura \ref{fig_pnljutten}.

Cada função não-linear $g_k$ (k=1,...,N) é modelado por redes MLP
com um neurônio linear na saída:
\begin{equation}\label{juttenpnl2}
    g_k (u)=\sum_{h=1}^{N_H} \xi^{h} \sigma (\omega^h u - \eta^h),
\end{equation}
onde $N_H$ é o número de neurônios ocultos. A divergência de
Kullback-Lieber é utilizada para encontrar as regras de aprendizado
para a estimação das funções não-lineares
\cite{article:jutten:1999}.

\begin{figure}[h!]
\centering
\includegraphics[width=6.5cm]{cap3_pnljutten}
\caption{Diagrama do algoritmo de Taleb-Jutten para o modelo PNL.}
\label{fig_pnljutten}
\end{figure}

Como existem vários parâmetros a serem ajustados no modelo inverso
proposto e a otimização envolve funções não-lineares, o algoritmo
pode apresentar problemas de convergência para mínimos locais
\cite{jutten:nlica:2003}. Diferentes procedimentos foram propostos
na literatura para melhorar a eficiência de estimação em modelos
PNL. Em \cite{article:nlica_ga:2001,article:kai:2006} um algoritmo
genético \cite{Book:Goldberg:1989} foi utilizado para executar uma
busca global, evitando o problema dos mínimos locais. O problema com
esta abordagem é o aumento do custo computacional.

Redes neurais com arquiteturas alternativas também foram aplicadas
com sucesso na separação de misturas PNL. Por exemplo, em
\cite{article:tan:20000} funções de base radial (RBF -
\textit{Radial Basis Function}). Em um outro trabalho
\cite{article:solazzi:pnl:2004}, a separação foi realizada por redes
neurais com funções de ativação do tipo \textit{spline}.
