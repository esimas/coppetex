\chapter{Técnicas Utilizadas para Extração de Características}

Neste capítulo, serão mostrados os fundamentos teóricos das técnicas de aprendizado estatístico utilizadas neste 
trabalho para extração de características na filtragem de segundo nível do ATLAS como uma extensão do discriminador 
Neural Ringer. Entre as técnicas de extração de características abordadas estão a análise de componentes 
independentes, em seus modelos não-linear e linear (NLICA e ICA), e para compactação, a análise de componentes 
principais (PCA) e os componentes principais de discriminação (PCD). Entre as técnicas citadas, maior destaque 
será dado a aquelas que exploram informações estatísticas de ordem superior (HOS - \textit{Higher-Order Statistics}), 
ou seja NLICA, ICA e PCD. Na Seção \ref{aplic} serão mostradas aplicações dos métodos listados acima em problemas de 
física de altas energias (HEP - \textit{High-Energy Physics}), juntamente com uma revisão bibliográfica das 
aplicações de redes neurais em HEP.


\section{Análise de Componentes Independentes}

Em muitos problemas de processamento de sinais multi-dimensionais deseja-se encontrar uma transformação que, de 
algum modo, torne a estrutura essencial dos dados mais acessível~\cite{book:oja:2001}. Em geral, não há muita 
informação disponível, e a busca pela nova representação dos sinais é feita através de aprendizado não-supervisionado. Entre as técnicas lineares que buscam, através de premissas distintas, por uma nova representação do conjunto de sinais pode-se mencionar a Análise de Componentes Principais (PCA - \textit{Principal Component Analysis})~\cite{book:pca:2002}, a Análise de Fatores (\textit{Factor Analysis})~\cite{book:factor:1967} e a Análise de Componentes Independentes (ICA - \textit{Independent Component Analysis})~\cite{book:oja:2001}. 

Entre as técnicas listadas acimas, a análise de componentes independentes (ICA - \textit{Independent Component Analysis}) busca por uma transformação onde os componentes na saída são estatisticamente independentes. A ICA vêm 
sendo aplicada na solução de diversos problemas na área de processamento de sinais como cancelamento de 
ruído~\cite{article:ica:noisecan}, sonar passivo~\cite{article:moura:sonarbook}, 
telecomunicações~\cite{article:icateleco:2007}, reconhecimento facial~\cite{article:icaface:2007} e 
biomédica~\cite{article:icabio:2007}. 

\begin{itemize}
\item \textbf{Independência Estatística}: Considerando duas variáveis
aleatórias (VAs) $y_1$ e $y_2$, se elas são independentes, então o
conhecimento de uma não traz nenhuma informação a respeito da outra. Matematicamente, $y_1$ e $y_2$ são 
independentes estatisticamente se e somente se \cite{Book:Papoulis:1991}:
\begin{equation}\label{indep1}
    p_{y1,y2}(y_1,y_2)=p_{y1}(y_1)p_{y2}(y_2),
\end{equation}
\\
onde $p_{y1,y2}(y_1,y_2)$, $p_{y1}(y_1)$ e $p_{y2}(y_2)$ são
respectivamente as funções de densidade de probabilidade (pdf -
\textit{probability density function}) conjunta e marginais de $y_1$
e $y_2$ \cite{Book:Papoulis:1991}. O conceito de independência envolve o conhecimento de toda
a estatística dos dados, sendo assim muito mais abrangente que a
descorrelação (utilizada pela PCA), que somente utiliza estatística
de segunda ordem (variância).

Pode-se obter uma expressão equivalente à equação (\ref{indep1}) se, para todas as
funções $g(y_1)$ e $h(y_2)$ absolutamente integráveis em $y_1$ e $y_2$, vale
a igualdade:
\begin{equation}\label{indep2}
    \mathcal{E}\{g(y_1)h(y_2)\}=\mathcal{E}\{g(y_1)\}\mathcal{E}\{h(y_2)\}
\end{equation}
\\
Para evitar a estimação das funções de densidade de probabilidade,
pode-se utilizar a equação (\ref{indep2}). A definição de
independência pode ser facilmente estendida para mais de duas
variáveis aleatórias. Percebe-se da equação (\ref{indep2}) que a
independência é um princípio mais restritivo que a descorrelação
(quando $g(x)=x$ e $h(y)=y$). No Apêndice \ref{apend_fex} serão
detalhados os princípios matemáticos mais utilizados para a
estimação dos componentes independentes.
\end{itemize}

Na ICA, considera-se que um sinal multi-dimensional
$\mathbf{x}(t)=[x_1(t),...,x_N(t)]^T$ observado (ou medido) é gerado
a partir da combinação linear das fontes independentes
$\mathbf{s}(t)=[s_1(t),...,s_N(t)]^T$. Na forma matricial e
suprimindo-se o índice temporal t, pode-se escrever
\cite{book:amari:2002}:
\begin{equation}\label{icamatrix}
\mathbf{x}=\mathbf{As},
\end{equation}
\\
onde $\mathbf{A}$ (N$\times$N) é a matriz de mistura.

O objetivo final da ICA é encontrar uma aproximação $\mathbf{y}$ das
fontes independentes ou da transformação linear $\mathbf{A}$,
utilizando apenas os sinais observados $\mathbf{x}$. O vetor
$\mathbf{y}$ é definido por:
\begin{equation}\label{ica_inv}
 \mathbf{y}=\mathbf{Wx}
\end{equation}
sendo $\mathbf{W}$ a matriz de separação.

Um problema clássico que pode ser solucionado usando-se ICA é conhecido
como \textit{cocktail-party problem}~\cite{book:oja:2001}, e está formulado de forma
simplificada, omitindo atrasos temporais e outros fenômenos físicos,
como a existência de múltiplas reflexões, nas equações (\ref{cpp}) e
(\ref{ccp2}) (ver Figura \ref{cocktail}). Considerando que numa sala
existem duas pessoas falando simultaneamente e dois microfones em
diferentes posições, os sinais gravados $x_1(t)$ e $x_2(t)$ são uma
soma ponderada das fontes $s_1(t)$ e $s_2(t)$:
\begin{eqnarray}\label{cpp}
% \nonumber to remove numbering (before each equation)
  x_1(t)=a_{11}s_1(t)+a_{12}s_2(t) \\
  \label{ccp2} x_2(t)=a_{21}s_1(t)+a_{22}s_2(t);
\end{eqnarray}
\\
os coeficientes $a_{ij}$ dependem das distâncias dos microfones às
pessoas, e podem ser considerados como os elementos da matriz de mistura $\mathbf{A}$ do
modelo da equação \ref{icamatrix}, onde:

\begin{equation}\label{matrizA}
    \mathbf{A}=\left(
                 \begin{array}{cc}
                   a_{11} &a_{12} \\
                   a_{21} & a_{22} \\
                 \end{array}
               \right).
\end{equation}
\\
Se os fatores $a_{ij}$ são conhecidos, o problema é facilmente
resolvido a partir de:
\begin{equation}\label{icamatrix2}
\mathbf{s}=\mathbf{Wx},
\end{equation}
\\
onde $\mathbf{W=A}^{-1}$. Na prática, tanto as fontes $s_i$ como os fatores
$a_{ij}$ devem ser obtidos apenas dos sinais misturados $x_i$.

\begin{figure}[th]
\centering
\includegraphics[width=9cm]{cap3_cocktail}
\caption{Diagrama do \textit{cocktail party problem}.}
\label{cocktail}
\end{figure}

Em um exemplo de aplicação de ICA, a Figura \ref{sinais}-a mostra as
fontes $s_1(t)$ e $s_2(t)$, que foram misturadas linearmente, gerando
os sinais $x_1(t)$ e $x_2(t)$ da Figura \ref{sinais}-b. Após a
aplicação de um algoritmo para extração das componentes
independentes (FastICA \cite{article:hyvarinen:2000}), foram obtidas
as curvas da Figura \ref{sinais}-c. Percebe-se que os sinais
recuperados são cópias dos originais, a menos de fatores
multiplicativos. Esta é uma das limitações inerentes do modelo da
ICA, não há como garantir o fator de escala (que pode ser positivo
ou negativo) ou a ordem de extração dos componentes.

\begin{figure}[h!]
\begin{minipage}[b]{.48\linewidth}
  \centering
 \centerline{\epsfig{file=cap3_fontes,width=7.5cm}}
  \vspace{.3cm}
  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
 \centerline{\epsfig{file=cap3_mix,width=7.5cm}}
  \vspace{.3cm}
  \centerline{(b)}\medskip
\end{minipage}
\hfill \linebreak
\begin{minipage}[b]{0.98\linewidth}
  \centering
 \centerline{\epsfig{file=cap3_rec,width=7.5cm}}
  \vspace{.3cm}
  \centerline{(c)}\medskip
\end{minipage}
\caption{Sinais (a) fonte, (b) observados e (c) recuperados através
da ICA.} \label{sinais}
\end{figure}

%\begin{figure}[t!]
%\begin{center}
%\subfigure[]{\label{fontes}\epsfig{file=cap3_fontes,width=7.5cm,clip=}}
%\subfigure[]{\label{misturas}\epsfig{file=cap3_mix,width=7.5cm,clip=}}
%\subfigure[]{\label{recuperados}\epsfig{file=cap3_rec,width=7.4cm,clip=}}
%\end{center}
%\caption{Sinais (a) fonte, (b) observados e (c) recuperados através
%da ICA.}
%\end{figure}

As técnicas de ICA foram desenvolvidas inicialmente para solucionar
pro\-blemas de separação cega de sinais (BSS - \textit{Blind Signal
Separation}) semelhantes ao \textit{cocktail-party problem}, porém,
mais recentemente, surgiram outras aplicações interessantes, como
extração de características, separação de fontes em telecomunicações
e redução de ruído em imagens
\cite{book:oja:2001,article:hyvarinen:2000}. Atualmente, ICA é aplicada com sucesso tanto para separação de 
sinais como para
extração de características.

\section{ICA não-linear}

Em muitos problemas práticos, o modelo básico da ICA, onde os sinais
observados são considerados combinações lineares e instantâneas das
fontes, não representa corretamente o cenário real.

A equação (\ref{nlica}) apresenta um modelo geral para as misturas
não-lineares:
\begin{equation}\label{nlica}
    \mathbf{x}=\mathbf{F}(\mathbf{s}),
\end{equation}
\\
onde $\mathbf{F}$ é um mapeamento não-linear de $\mathbb{R}^N
\rightarrow \mathbb{R}^N$, $\mathbf{x}$ e $\mathbf{s}$ são respectivamente os sinais observados
e as fontes. A ICA não-linear consiste em encontrar o mapeamento
$\mathbf{G}$: $\mathbb{R}^N \rightarrow \mathbb{R}^N$ tal que os
componentes de $\mathbf{y}$ sejam estatisticamente independentes~\cite{article:simas:2007:lnlm}:
\begin{equation}\label{nlica2}
    \mathbf{y}=\mathbf{G}(\mathbf{x}).
\end{equation}

Uma característica da NLICA é que o problema apresenta múltiplas
soluções~\cite{jutten:nlica:2003}. Se $\mathbf{y_1}$ e
$\mathbf{y_2}$ são variáveis aleatórias independentes, é fácil
provar que $f(\mathbf{y_1})$ e $g(\mathbf{y_2})$ (onde $f(.)$ e
$g(.)$ são funções diferenciáveis) são também independentes \cite{article:hyvarinen:1999}.
Fica claro que, sem o uso de alguma restrição, infinitos mapeamentos
inversos $\mathbf{G}$ satisfazem a condição de independência entre os sinais
estimados $y_i$, i=1,...,N, em uma dada aplicação. Se o objetivo do
problema for realizar a separação cega das fontes (BSS -
\textit{Blind Signal Separation}), neste caso, deseja-se que as
componentes $y_i$ sejam as fontes independentes que produziram os
sinais observados $\mathbf{x}$. Então, informações a respeito do
modelo de mistura ou das fontes devem ser conhecidas a priori. A
NLICA vem sendo aplicada com sucesso em problemas como processamento
de sinais de fala \cite{article:rojas:ica:2003,article:miyabe:2009}, processamento de 
imagens~\cite{article:allinson:2002,article:almeida:2008}, predição de séries de ações em bolsas de 
valores~\cite{article:chi:2009} e processamento de sinais de um \textit{array} de 
sensores~\cite{article:sensornlica:2007,app:nlica:qui}.

Em geral, o número de parâmetros a serem estimados num modelo de ICA
não-linear é maior do que no caso linear. Os algoritmos de NLICA, se
comparados aos de ICA, apresentam maior complexidade
computacional e convergência mais lenta \cite{jutten:nlica:2003}. Em problemas de separação
cega de fontes, o algoritmo a ser utilizado deve ser esco\-lhi\-do
utilizando informações a respeito do modelo de mistura. Considerando
estas limitações, as aplicações de NLICA devem considerar se existem restrições quanto ao aumento do tempo de
processamento na estimação dos componentes.

Entre os algoritmos de NLICA propostos na literatura, uma classe de
métodos impõe restrições estruturais ao modelo de mistura. Neste
caso, pode-se garantir que os componentes estimados são iguais às
fontes (a menos das indeterminações de fator multiplicativo e ordem
de estimação dos sinais, assim como ocorre no modelo linear). Uma solução
de implementação mais direta é o uso de mapas auto-organizáveis para
estimar o mapeamento não-li\-near, neste caso não há restrição de
modelo. Outro método diretamente relacionado com a NLICA, chamado de
ICA Local, propõe uma etapa de agrupamento dos sinais em conjuntos
de características semelhantes, que deve ser realizada antes da ICA.
O agrupamento produz um mapeamento não-linear dos dados e a ICA
(linear) estima os componentes independentes. Mais informações a
respeito dos diversos algoritmos e modelos de NLICA serão fornecidas
nas próximas seções.

\subsection{Unicidade da Solução em NLICA} \label{seq_uni}

No caso não-linear, a independência estatística não é suficiente
para garantir a sepa\-ração das fontes. Se duas variáveis aleatórias
$y_1$ e $y_2$ são independentes, então
$p_{y1,y2}(y_1,y_2)=p_{y1}(y_1)p_{y2}(y_2)$. Para funções
diferenciáveis $f$ e $g$, pode-se provar
que~\cite{article:jutten:1999}:
\begin{equation}\label{uniq}
    p_{f(y_1),g(y_2)}(y_1,y_2)=p_{f(y_1)}(y_1)p_{g(y_2)}(y_2),
\end{equation}
e então as variáveis $f(y_1)$ e $g(y_2)$ são também independentes.
Esta indeterminação, diferente do fator de escala e da ordem de
estimação das componentes (que são inerentes a ICA linear), não são
aceitáveis em um problema de separação de fontes.

Estudos teóricos indicaram que a unicidade da solução da NLICA pode
ser conseguida se o problema apresentar pelo menos uma das
características a seguir \cite{article:hyvarinen:1999}:

\begin{itemize}
    \item O número de componentes é igual a dois. Deste modo, os sinais podem ser considerados como uma variável complexa.
    \item As pdf das componentes independentes são limitadas a valores conhecidos.
    \item A função de mistura $\mathbf{F}$ preserva o zero ($\mathbf{F}(0)=0$) e é um mapeamento unívoco que preserva localmente a ortogonalidade das coordenadas.
    \item O modelo de mistura é conhecido a priori e utilizado como informação para o algoritmo de estimação das componentes independentes.
\end{itemize}


\subsection{Misturas Pós Não-Lineares}

Um caso especial da ICA não-linear são os métodos de estimação que
incluem no algoritmo de estimação dos componentes independentes
informações a respeito do modelo não-linear que gerou os dados
observados. Estas informações se configuram em restrições
estruturais ao mapeamento inverso (que é estimado pelo algoritmo).
Entre estes modelos, o de misturas \textbf{pós não-lineares} (PNL)
\cite{article:jutten:1999} é um dos mais utilizados na literatura, com aplicações em processamento de 
fala~\cite{article:rojas:nlica:2004,ica:ap_pnl:2004}, separação de sinais de áudio~\cite{ica:ap_pnl:2007,article:solazzi:pnl:2004}, 
processamento de imagens \cite{article:pnlimage:2007} and ...
... (ver algo mais nos proceedings do ICA)... 

No modelo PNL, considera-se que inicialmente ocorre uma combinação
linear das fontes (como no modelo básico de ICA), e as funções
não-lineares $f_i$ são aplicadas antes da observação dos sinais
$x_i$:
\begin{equation}\label{pnlin}
    x_i=f_i\Big(\sum_{j=1}^n a_{ij}s_j \Big).
\end{equation}
É importante notar que as não-linearidades são aplicadas
individualmente a cada componente da mistura linear (não são
permitidas linearidades cruzadas). A Figura~\ref{fig_pnl} ilustra o
modelo de misturas PNL.

\begin{figure}[th]
\centering
\includegraphics[width=10cm]{cap3_pnl}
\caption{Diagrama do modelo de mistura PNL.} \label{fig_pnl}
\end{figure}

A consideração de que as misturas são pós não-lineares permite uma
grande simplificação do problema, e as indeterminações existentes se
tornam semelhantes às do caso linear. A modelagem através da equação
(\ref{pnlin}) satisfaz grande parte dos fenômenos não-lineares,
como, por exemplo, a modelagem da distorção de sensores num meio de
propagação linear.

Detalhes de alguns algoritmos para estimação dos componentes
independentes em misturas PNL são fornecidos no Apêndice
\ref{apend_fex}.

\subsection{Outros modelos de misturas com restrições estruturais}

Alguns modelos com restrições estruturais diferentes do PNL foram
propostos na literatura. No trabalho
\cite{article:solazzi:pnl:2004}, o modelo de mistura é definido por:
\begin{equation}\label{eq_pnll}
    \mathbf{x}=\mathbf{A_2}f(\mathbf{A_1}\mathbf{s}),
\end{equation}
sendo $\mathbf{A_1}$ e $\mathbf{A_2}$ matrizes quadradas e
$f=[f_1,f_2,...,f_N]^T$ são funções não-lineares aplicados a cada
componente (assim como o modelo PNL, este também não permite
não-linearidades aplicadas a mais de um componente). O modelo
definido na Equação \ref{eq_pnll} e ilustrado também na Figura
\ref{fig_pnll} é chamado Pós Não-linear Linear (PNL-L). O bloco
linear $\mathbf{A_2}$ é executado após a aplicação das funções
não-lineares, produzindo um modelo mais geral que o PNL. Nos
trabalhos \cite{article:woo:nl2005,article:gao:nl2005} são propostos
algoritmos baseados em redes neurais para a estimação do modelo
PNL-L.

\begin{figure}[th]
\centering
\includegraphics[width=7cm]{cap3_pnll}
\caption{Diagrama do modelo PNL-L.} \label{fig_pnll}
\end{figure}

Em \cite{article:gao:nl2005}, um modelo estrutural chamado mono
não-linearidade (ver Figura \ref{fig_mnlin}) foi proposto para o
problema da NLICA. Neste modelo os sinais observados são gerados a
partir de:
\begin{equation}\label{eq_mnlin}
    \mathbf{x}=f^{-1}(\mathbf{A}f(\mathbf{s})).
\end{equation}
Este modelo é dito mais geral que o PNL pois as funções não-lineares
podem ser aplicadas a mais de um componente (as funções $f_i$ podem
ser funções não-lineares de mais de uma variável). Este modelo é
chamado de mistura de mono não-linearidade (ver Figura
\ref{fig_mnlin}). A generalidade deste modelo é derivada da teoria
da análise funcional (\textit{functional analysis})
\cite{book:function:1984} e foi mostrado em
\cite{article:gao:nl2005} que esta arquitetura pode representar
qualquer mistura com duas camadas de não-linearidades.

\begin{figure}[th]
\centering
\includegraphics[width=8.5cm]{cap3_mononl}
\caption{Diagrama do modelo da Mono não-linearidade.}
\label{fig_mnlin}
\end{figure}

\subsection{Algoritmos sem restrições estruturais}

Se nenhuma restrição ao modelo de mistura é imposta, não há garantia
que os componentes independentes estimados estejam relacionados com
as fontes (ver Seção \ref{seq_uni}). 
Entre os métodos de NLICA sem restrições estruturais, pode-se
destacar o uso de mapas auto-organizáveis~\cite{article:pajunen:1996} e os
métodos que utilizam inferência Bayesiana
\cite{article:lappalainen:2000}.

\subsubsection{NLICA a partir de Mapas Auto-Organizáveis}:

Uma das primeiras tentativas bem sucedidas de realizar NLICA
utilizou mapas auto-organizáveis \cite{article:pajunen:1996}. Outras aplicações incluem: 
... citar aplicações ...


Pode-se provar que as coordenadas $y_1$ e $y_2$ do neurônio vencedor
no mapa (ver Figura \ref{fig_som_nlica}) são independentes e
aproximadamente uniformemente distribuídas
\cite{article:pajunen:1996}. Para estimar a NLICA, o SOM é treinado
usando como entradas os sinais observados, e as coordenadas do
neurônio vencedor correspondem a uma aproximação dos componentes
independentes.

\begin{figure}[th]
\centering
\includegraphics[width=8cm]{cap3_som_nlica}
\caption{NLICA a partir de SOM.} \label{fig_som_nlica}
\end{figure}

Entre as desvantagens do método pode-se destacar \cite{jutten:nlica:2003}:

\begin{itemize}
    \item O mapeamento é discreto (existe um número limitado de neurônios no mapa),
    então, algum tipo de regularização é necessária para produzir componentes contínuos.
    Esse problema pode ser minimizado aumentando-se o número de neurônios do mapa.
    \item Os componentes a serem estimados devem ter pdf sub-gaussiana (quanto mais próxima da distribuição uniforme, melhor).
    \item O custo computacional para treinamento dos mapas aumenta rapidamente com o número de componentes independentes a serem estimados.
\end{itemize}

Para avaliar o custo computacional, o número de parâmetros Np do SOM
pode ser estimado pela expressão:
\begin{equation}
    Np=N \times (Q_L)^N
\end{equation}
onde $N$ é o número de componentes a serem estimados (que é
considerado igual ao número de sinais observados) e $Q_L$ é o número
de níveis de quantização desejado \cite{article:simas:2007:lnlm}.

Com uma formulação alternativa aos SOM, o Mapeamento Topográfico
Generativo (GTM - \textit{Generative Topographic Mapping}) foi
introduzido em \cite{article:bishop:1998}, e apresenta princípios
estatísticos mais fundamentados que o mapa SOM. O método GTM básico
tem poucas vantagens práticas em relação aos Mapas
Auto-Organizáveis, pois aqui as componentes independentes também são
assumidas como processos uniformemente distribuídos e o espaço de
características é formado a partir de uma grade retangular discreta
m-dimensional. Porém, devido a sua formulação matemática mais
fundamentada, o GTM pode ser estendido para variáveis não
uniformes. O trabalho \cite{article:pajunen:1997} propõe uma
modificação à formulação básica onde são introduzidos
coe\-fi\-cientes de ponderação que permitem a estimação de
componentes independentes com qualquer tipo de distribuição. Os
componentes são modelados como misturas de sinais Gaussianos e os
parâmetros são estimados usando o algoritmo \textit{Expectation
Maximization} \cite{article:hyvarinen:2000}. O treinamento do GMT
envolve dois passos, a avaliação da probabilidade a
\textit{posteriori} e a adaptação dos parâmetros do modelo.

\subsubsection{NLICA a partir de Inferência Bayesiana}:

Nos métodos baseados em inferência Bayesiana, considera-se que os
sinais observados são gerados a partir de
\cite{article:lappalainen:2000}:
\begin{equation}
    \mathbf{x}=f(\mathbf{s}) + \mathbf{n}
\end{equation}
onde $\mathbf{n}$ é definido como ruído Gaussiano independente dos
componentes a serem estimados.

Os componentes independentes são modelados como misturas de sinais
de distribuição Gaussiana. Pode-se provar que, dado um número suficiente de
Gaussianas, virtualmente qualquer distribuição de probabilidade pode
ser modelada com uma certa precisão~\cite{article:lappalainen:2000}. Uma variação deste método foi
aplicada em \cite{article:lappalainen:bay1999} para o modelo linear
da ICA. Em grande parte dos algoritmos Bayesianos para NLICA~\cite{}, redes
neurais tipo MLP de duas camadas são treinadas para aproximar o
mapeamento não linear:
\begin{equation}
    f(\mathbf{s})=\mathbf{B}\Phi(\mathbf{As}+\mathbf{a})+\mathbf{b}
\end{equation}

Em um método de estimação Bayesiano, probabilidades a
\textit{posteriori} são associadas a cada modelo não-linear que,
possivelmente, teria gerado os dados observados. Verificar uma
quantidade tão grande de modelos não é possível na prática; então, os
métodos Bayesianos para NLICA utilizam uma técnica chamada de
``aprendizagem amostral" (EL - \textit{ensemble learning})
\cite{aricle:minskin:2000}. Na EL, somente o conjunto mais provável
de modelos é testado utilizando uma aproximação paramétrica que é
ajustada à probabilidade a \textit{posteriori}
\cite{article:valpola:2000ica}.

Métodos Bayesianos de NLICA foram propostos
em~\cite{article:honkela:2004ica} e \cite{article:honkela:2007}. No
trabalho~\cite{article:ilin:2004ie} foram realizados testes
experimentais para comparar o desempenho dos mode\-los Bayesiano e
PNL na estimação dos componentes independentes, as principais
conclusões foram:
\begin{itemize}
    \item os algoritmos PNL apresentam desempenho superior quando as misturas
     seguem o modelo PNL clássico (não-linearidades inversíveis e mesmo número de componentes independentes e
     sinais observados);
    \item o desempenho de ambos os métodos pode ser melhorada a partir da
    exploração da informação de mais misturas que componentes independentes;
    \item a principal vantagem do método Bayesiano é que mapeamentos mais
    genéricos podem ser produzidos (uma vez que não há restrições estruturais).
    Estes métodos geralmente apresentam maior custo computacional e necessitam
    de várias inicializações para obter uma solução ótima (podem apresentar
    problemas com mínimos locais da função custo).
\end{itemize}

... Citar aplicações ...

\subsubsection{O algoritmo MISEP}

O algoritmo MISEP utiliza a minimização da Informação Mútua (ver Apêndice \ref{apend_fex}) como estratégia para 
busca pelos componentes independentes, e é considerado como uma extensão do método INFOMAX \cite{book:oja:2001}, 
podendo ser utilizado para estimar tanto o modelo linear quanto o não-linear da ICA. Na Figura \ref{fig_misep} 
pode-se observar um diagrama do MISEP, onde $y_i$ são os componentes independentes 
estimados e o bloco $\mathbf{F}$, no caso linear, aproxima a matriz de separação $\mathbf{W}$, e para a NLICA, deve 
fornecer uma aproximação do mapeamento inverso $G(.)$. As funções não-lineares $\psi_i$ e as variáveis de saída 
$z_i$ são utilizadas apenas no processo de 
treinamento. Após a aonvergência do algoritmo, as não-linearidades deveriam ser aproximações da função de 
probabilidade cumulativa (cdf - \textit{cumulative probability function}) dos componentes independentes. 

Para a aplicação em NLICA, o bloco $\mathbf{F}$ é estimado por uma rede neural (que pode utilizar tanto a arquitetura 
\textit{perceptron} de múltiplas camadas - MLP como rede de funções de base radial - RBF). Como o objetivo é estimar 
a cdf, as saídas $z_i$ são restritas ao intervalo [0,1] e as $\psi_i$ são limitadas 
a funções estritamente crescentes. Para estimação iterativa de cada função $\psi_i$ é utilizada uma redes neurais MLP com 
uma camada oculta (de neurônios sigmoidais) e uma camada de saída (linear). Estas redes tem uma entrada ($y_i$) e uma 
saída ($z_i$). O treinamento do modelo MISEP é feito a partir da maximização da entropia das saídas $z_i$, mais 
detalhes podem ser encontrados em \cite{}

O MISEP foi aplicado em processamento de sinais de áudio \cite{} e separação de imagens \cite{}. No trabalho \cite{}, 
foi proposta uma modificação no MISEP para...

\subsection{ICA Local}

Se o modelo da ICA for utilizado para extração de características
(ao invés de sepa\-ração de fontes), uma melhor descrição dos dados
pode ser obtida se forem exploradas características locais.
Considerando um conjunto de sinais multi-dimensionais com
estatística variável, o modelo da ICA linear pode não ser capaz de
revelar a estrutura fundamental dos dados. Neste caso, é mais
razoável realizar a extração de características (estimação das
componentes independentes) a partir de k subconjuntos dos dados (ver
Figura \ref{fig_local}). Os sinais pertencentes ao k-ésimo
subconjunto apresentam características semelhantes. Este
procedimento leva ao modelo da ICA local.

\begin{figure}[th]
\centering
\includegraphics[width=8.5cm]{cap3_local}
\caption{Diagrama do modelo da ICA local.} \label{fig_local}
\end{figure}

Conforme proposto em \cite{karhunen:local:1999}, um conjunto de
dados de alta dimensão pode ser separado em sub-conjuntos, através
de algum algoritmo de agrupamento como o \textit{k-means}
\cite{book:duda:2000} ou SOM \cite{article:som:oja:1996}, e
componentes independentes lineares são então estimadas de cada
subconjunto. Determinar o número ideal de agrupamentos em um
conjunto de dados não é uma tarefa simples e, geralmente, requer
informação a priori~\cite{}. 

Na ICA Local, o agrupamento é responsável por uma representação
não-linear dos dados, enquanto modelos de ICA linear aplicados a
cada sub-conjunto (\textit{cluster}) descrevem as características
locais dos dados. A ICA local pode ser considerada como um
compromisso entre os modelos linear e não-linear da ICA
\cite{jutten:nlica:2003}. O objetivo é obter uma melhor
representação dos dados (se comparado com o modelo linear da ICA),
evitando os problemas computacionais do modelo não-linear
\cite{karhunen:local:2000}. Em diferentes abordagens, os
agrupamentos podem ser montados com superposição, usando por exemplo
fronteiras \textit{fuzzy}
\cite{article:honda:2000,article:honda:2006}, ou sem superposição
\cite{karhunen:local:2000,aricle:palmieri:2000}.

Nos trabalhos \cite{article:lan:2005,article:lan:2006} a ICA Local
foi aplicada para a estimação da informação mútua. A informação
mútua \cite{book:cover:1991} é uma importante ferramenta em diversas
aplicações de processamento de sinais, especialmente na seleção de
ca\-rac\-terísticas.

... Citar aplicações da Local ICA ...

\section{Mapas auto-organizáveis}

O mapa auto-organizável (SOM - \textit{Self Organizing Map}) é uma
rede neural com treinamento não-supervisionado, baseado na
aprendizagem competitiva, que é capaz de realizar uma organização
topológica das entradas. O SOM foi proposto por Teuvo Kohonen em
1982 \cite{book:kohonen:2001}, sendo capaz de realizar um
mapeamento não-linear dos sinais de um espaço de entrada contínuo de
dimensão $k$ para um espaço de características discreto que, em
geral, é bidimensional. Cada neurônio da grade está diretamente
conectado a todos os nós de entrada. Na Figura \ref{soms} pode-se
visualizar o diagrama de um mapa auto-organizável bi-dimensional.

\begin{figure}[h!]
\centering
\includegraphics[width=6cm]{cap3_som}
\caption{Diagrama de um mapa auto-organizável} \label{soms}
\end{figure}

O mapa auto-organizável compacta a informação e preserva relações
topológicas ou métricas do conjunto de sinais. Os SOM estão ligados
à ICA por conseguirem extrair informações ocultas dos sinais de
forma não supervisionada \cite{article:foo:2006}. Uma apro\-xi\-mação
das componentes independentes não-lineares pode ser obtida
utilizando mapas auto-organizáveis \cite{book:oja:2001}.

Três processos estão envolvidos na formação do mapa auto-organizado:
a \textbf{competição}, onde, para cada vetor de entrada, há apenas um
neurônio vencedor; a \textbf{cooperação}, quando o neurônio vencedor
determina uma vizinhança topológica de neurônios excitados; e a
\textbf{adaptação}, que procede ao ajuste dos pesos sinápticos para
reforçar a resposta do neurônio vencedor, e de seus vizinhos, ao
padrão de entrada.

A atualização do vetor de pesos $w_j$ do neurônio j é feita através da equação:
\begin{equation}\label{pesos_som}
    w_j(n+1)=w_j(n)+\eta (n) h_{ij}(n)(x(n)-w_j(n)),
\end{equation}
\\
sendo $\eta (n)$ a taxa de aprendizagem, um tipo de função de
vizinhança $h_{ij}(n)$ usualmente utilizada é definida por:
\begin{equation}\label{neigbor}
h_{ij}(n)=\exp(-d_{ij}^2/2\sigma^2(n))
\end{equation}
\\
onde $d_{ij}$ é a distância do neurônio j para o neurônio vencedor i
e $\sigma(n)$ é a largura da função vizinhança na n-ésima iteração.

O mapa de características possui algumas propriedades, listadas a
seguir \cite{haykin:nn:2008}:

\begin{enumerate}
  \item é formado pelo conjunto de
vetores de pesos sinápticos $w_i$ no espaço de saída discreto e
fornece uma boa aproximação para o espaço de entrada;

  \item é ordenado de modo
topológico. Padrões de entrada semelhantes são mapeados para regiões
adjacentes no mapa de características;

  \item regiões do espaço de entrada que possuem alta
probabilidade de ocorrência são mapeadas para domínios maiores do
espaço de saída;

\end{enumerate}

No mapa de características, o neurônio que apresentar maior saída é
consi\-derado o vencedor, ou seja a saída do SOM é do tipo ``vencedor
leva tudo" (WTA - \textit{winner takes all}). O neurônio ativado é
escolhido a partir de sua semelhança com a entrada apresentada. É
comum a utilização da distância euclidiana como métrica da
proxi\-midade entre dois vetores; nesse caso, o neurônio vencedor é
aquele que minimiza $i(\mathbf{x})=\| \mathbf{x}(n)-\mathbf{w_j}
\|$.

Uma outra forma de operar um mapa auto-organizável é utilizar as projeções dos sinais de entrada no mapa de 
características, ou seja as saídas $u_j$ de cada neurônio j que pode ser calculada por:
\begin{equation}\label{som_out}
 u_j=\mathbf{x}^T\mathbf{w_j}
\end{equation}
O vetor $\mathbf{u}=[u_1,...,u_K]^T$ pode ser considerado como a projeção de $\mathbf{x}$ no mapa de características.

Os mapas auto-organizáveis pertencem à classe de algoritmos de
codificação vetorial, sendo capazes de encontrar, de forma
otimizada, um número fixo de vetores ou palavras de código que
melhor representem o conjunto de sinais.

\section{Técnicas de Pré-Processamento - Compactação}

No processamento de sinais multi-dimensionais, é comum a utilização de técnicas de processamento de sinais 
que visam a redução da dimensionalidade do problema. O objetivo é projetar os sinais N-dimensionais observados em 

\subsection{Análise de Componentes Principais}

A análise de componentes principais (PCA - \textit{Principal
Component Analysis}) é uma técnica estatística de processamento de
sinais diretamente ligada à transformação de \textit{Karhunen-Loève}
\cite{book:pca:2002}. O objetivo da PCA é encontrar uma
transformação linear onde os sinais projetados sejam
não-correlacionados e grande parcela da energia (variância) esteja
concentrada num pequeno número de componentes. Para isso, são
exploradas informações da estatística de segunda ordem. 

A análise de
componentes principais é bastante usada para compactação de
informação. Como a PCA projeta os sinais em componentes ordenados por energia, uma métrica geralmente 
utilizada para reduzir a dimensão dos dados consiste na seleção apenas dos componentes de maior energia, 
de modo que o sinal recuperado a
partir da informação compactada tenha pequeno erro médio quadrático
se comparado ao original. A seguir serão desenvolvidos, de forma
resumida, os fundamentos matemáticos da PCA.

Considerando-se um vetor $\mathbf{x}=[x_1,...,x_N]^T$ aleatório com $N$ elementos,
assume-se que ele tenha média zero:

\begin{equation}\label{Xm}
    \mathcal{E}\{\mathbf{x}\}=0
\end{equation}
\\
onde $\mathcal{E}\{.\}$ é o operador esperança. Se $\mathbf{x}$ tem
média não nula faz-se $\mathbf{x}\leftarrow
\mathbf{x}-\mathcal{E}\{\mathbf{x}\}$.

A projeção $z_i$ de $\mathbf{x}$ na direção de $\mathbf{v}_i$ pode
ser expressa por:

\begin{equation}\label{proj1}
    z_i=\mathbf{v}_i^T\mathbf{x}=\sum_{k=1}^N v_{ki}x_k
\end{equation}
\\
Na transformação por PCA, os componentes $z_i$ ($i=1,...,N$) devem
ser ortogonais e ordenados (de modo decrescente) pela variância das
projeções, sendo, então, $z_1$ a projeção de máxima variância. Para
tornar a variância independente da norma de $\mathbf{v}_i$, faz-se:

\begin{equation}\label{normaw}
    \mathbf{v}_i \leftarrow \frac{\mathbf{v}_i}{\| \mathbf{v}_i \|}
\end{equation}
\\
Fazendo-se com que $||\mathbf{v}_i||=1$, torna-se a variância função apenas da direção
das projeções. 
%A ortogonalidade garante a não-correlação entre as
%componentes.

Como $\mathcal{E}\{\mathbf{x}\}=0$, então $\mathcal{E}\{z_i\}=0$,
logo a variância da projeção $z_i$ é calculada por
$\mathcal{E}\{z_i^2\}$. Seguindo a definição da PCA, $z_1$ tem
máxima variância; logo, $\mathbf{v}_1$ pode ser encontrado pela maximização
de \cite{book:oja:2001}:

\begin{equation}\label{w1pca}
    J_1^{PCA}(\mathbf{v}_1)=\mathcal{E}\{z_i^2\}=\mathcal{E}\{(\mathbf{v}_1^T\mathbf{x})^2\}
    =\mathbf{v}_1^T\mathcal{E}\{\mathbf{x}\mathbf{x}^T\}\mathbf{v}_1=
    \mathbf{v}_1^T \mathbf{C}_x\mathbf{v}_1,
\end{equation}
\\
onde $\mathbf{C}_x$ é a matriz de covariância de $\mathbf{x}$.

A solução para o problema de maximização da equação (\ref{w1pca}) pode
ser encontrada na álgebra linear, em função dos autovetores
$\mathbf{e}_1,\mathbf{e}_2,...,\mathbf{e}_N$ da matriz
$\mathbf{C}_x$. A ordem dos autovetores é tal que os autovalores
associados satisfazem $d_1>d_2>...>d_N$. Desta forma, tem-se:

\begin{equation}\label{pca1}
    \begin{array}{ccc}
      \mathbf{v}_i= \mathbf{e}_i,& & 1\leq i \leq N
    \end{array}
\end{equation}

Percebe-se que a PCA de $\mathbf{x}$ e a decomposição por
autovalores da matriz $\mathbf{C}_x$ (de dimensão $N\times N$) são
equivalentes. Limitações computacionais na extração das componentes
principais utilizando as equações (\ref{proj1}) e (\ref{pca1})
aparecem quando a dimensão $N$ do vetor $\mathbf{x}$ aumenta, pois o
processo de obtenção dos autovetores se torna proibitivamente lento.
Nesse caso, uma solução é utilizar métodos iterativos de extração
das componentes principais, através de redes neurais
\cite{article:oja:pca,tese:joao:2007}.

\section{Utilizando Informação das Classes na Estimação dos Componentes Independentes}

O modelo da ICA/NLICA não foi originalmente desemvolvido para extração de componentes com características 
discriminantes, então, neste trabalho foram propostas algumas modificações no processo de estimação da NLICA, de modo 
a utilizar o conhecimento prévio dos rótulos de classe dos dados. Assim, pretende-se...

\subsection{Componentes Principais de Discriminação}

Considerando um problema de classificação de padrões, o uso da PCA para compactação pode ser prejudicial, 
pois, os componentes menos energéticos (que são eliminados após a PCA) podem carregar informações discriminantes. 
Neste caso, pode-se utilizar técnicas de compactação mais adequadas. As componentes principais de discriminação 
(PCD - \textit{Principal Discriminanting Components})~\cite{seixas:pcd:1995,seixas:pcd:1999} são obtidas a partir 
da projeção dos sinais de entrada em um conjunto compacto que carrega toda a informação importante para 
discriminação entre as classes.

Conforme proposto em \cite{seixas:pcd:1995}, para um problema de classificação, o objetivo da PCD é obter uma projeção linear 
dos sinais de entrada $\mathbf{x}=[x_1,...,x_N]^T$ nos componentes $\mathbf{z}=[z_1,...,z_K]^T$ (com $K<N$) que 
maximizam a discriminação entre as classes (ou seja, $z_i$ são os componentes principais de discriminação). 

Considerando um problema de discriminação onde existem apenas duas classes possíveis, os PCD podem ser estimados a 
partir de uma rede neural (de arquitetura MLP - \textit{Multi-Layer Perceptron})~\cite{haykin:nn:2008} com uma 
camada oculta e um neurônio de saída, treinada para obter máxima discriminação entre as classes. Conforme indicado 
na Figura~\ref{fig_pcd}-a, uma rede neural com um neurônio na camada oculta é capaz de estimar o primeiro PCD, 
que é obtido a partir da projeção das entradas na direção dos pesos sinápticos do neurônio oculto:
\begin{equation}
 z_1=[b_{1,1},b_{1,2},...,b_{1,N}]\times[x_1,...,x_N]^T + b_{0,1},
\end{equation}
onde $b_{0,1}$ é o \textit{bias} do neurônio. Adicionando-se mais neurônios ocultos consegue-se estimar os 
demais PCD (conforme ilustrado na Figura~\ref{fig_pcd}-b). No processo de estimação, estatística de ordem 
elevada é acessada a partir da utilização de funções de ativação não-lineares. O treinamento da rede neural 
pode ser feito com o congelamento dos pesos da camada de entrada correspondentes aos componentes já estimados, 
ou seja, na estimação do $l$-ésimo componente, os pesos $b_{i,j}$, com $i<l$ e $j=1,...,N$ não são ajustados. 
Os demais pesos da rede são ajustados a cada novo componente estimado.

Outros modelos que, de modo semelhante aos PCD, utilizam redes neurais para extrair características 
discriminantes de um conjunto de sinais foram propostos na literatura em 
\cite{article:nnfex1:1996,article:nnfex2:1996,article:nnfex:1999}.

\subsection{Utilizando os Rótulos de Classe como Sinais de Entrada}

No trabalho \cite{} foi proposta a utilização dos rótulos de classes como entrada para ...

\subsection{Algoritmo PNL modificado}

... descrever o algoritmo proposto no artigo Neurocomputing ...

\section{Aplicações}
\label{aplic}

Nessa Seção serão descritas, de modo resumido, algumas aplicações da análise de componentes independentes, nos seus 
modelos linear e não-linear (respectivamente ICA e NLICA), para a extração de características.

No trabalho \cite{ica:clas:2004}, ICA foi utilizada como pré-processamento para problemas de classificação em nove bases 
de dados diferentes (obtidas no repositório de bases de dados para aprendizado de máquina da Universidade da 
Califórnia - Irvine, CA, Estados Unidos \cite{rep:uci:1998}). Entre os problemas testados, estão a classificação 
de vinhos a partir de 
características físicas e químicas, a identificação da existência de câncer em amostras de tecido 
da mama, a identificação isolada de vogais independente do locutor e previsão de sobrevida de pacientes que sofreram ataque cardíaco a partir do resultado do eletrocardiograma.
A transformação da ICA foi estimada através do algoritmo JADE~\cite{article:Cardoso:1993}. Utilizando-se classificadores neurais (MLP), a eficiência 
foi comparada para sinais sem pré-processamento, sinais branqueados e sinais após ICA. Em alguns casos (como na 
identificação de vogais) o uso da ICA produziu uma redução do erro de identificação (24,13$\%$ sem pré-processamento, 
21,05$\%$ após o branqueamento e 20,77$\%$ após a ICA). Em outros casos, porém, a aplicação da ICA tornou mais difícil 
o problema de classificação (como no caso da identificação do cancer de mama, onde, sem pré-processamento, o erro foi 
de 2,55$\%$ e após a ICA aumentou para 2,63$\%$). Analisando-se todos os resultados conclui-se que, nem sempre a 
aplicação da ICA contribui para um aumento na eficiência, esse fato é intensificado em problemas onde o modelo da 
mistura linear não se aplica (pois possivelmente existem não-linearidades envolvidas). O uso da ICA parece tornar 
mais suave a curva de erro de treinamento das redes neurais, contribuindo para a diminuição 
da quantidade de mínimos locais e, consequentemente, da probabilidade do treinamento ficar estacionado num desses 
mínimos.

Em \cite{article:icabreast:2005}, ICA foi novamente utilizada para detecção do cancer de mama a partir de imagens digitalizadas de 
mamografias. Nesse trabalho, os componentes independentes foram estimados através do algoritmo 
FastICA~\cite{article:fastica:1999} e classificadores neurais (MLP) foram utilizados para produzir a decisão. 
As amostras disponíveis pertenciam a três classes distintas (normais, com alterações 
benignas e com alterações malignas). A ICA foi estimada a partir de pequenas janelas nas mamografias onde as classes de 
interesse eram mais facilmente identificadas. Foram obtidas eficiências de identificação da ordem de 99,9$\%$ para 
as amostras normais, 86,8$\%$ e 91,1$\%$ respectivamente para amostras com alterações benignas e malignas.

Microarranjos de DNA foram pré-processados por ICA em \cite{article:icamicroa:2006} para a classificação através de máquinas de vetor de 
suporte (SVM - \textit{Support Vector Machines}) \cite{haykin:nn:2008}. Os microarranjos de DNA são fragmentos 
genômicos que representam segmentos gênicos em particular. Nesse trabalho, o algoritmo FastICA foi utilizado para 
extrair características dos microarranjos (de quatro bases de dados distintas) com o objetivo de identificar a 
presença de diferentes tipos de tumores (de colo de útero, leucemia, de fígado e do sistema nervoso). As eficiências 
de identificação obtidas para os quatro tipos foram, respectivamente, 90$\%$, 100$\%$, 74$\%$ e 76$\%$. Uma outra 
aplicação de ICA no mesmo problema pode ser encontrada em \cite{article:icamicroa:2009}.

Ainda na área biomédica, no trabalho \cite{article:icaglauc:2005}, a ICA foi utilizada como pré-processamento para um mapeamento 
não-supervisonado de características oculares, com o objetivo de identificar a presença de glaucoma. A partir de 
padrões de um exame conhecido como \textit{standard automated perimetry} (SAP), aplicou-se a ICA e o agrupamento 
(não-supervisonado) foi realizado sobre os componentes independentes estimados. Através dessa 
abordagem, 98,4$\%$ das assinaturas de olhos com padrão óptico normal foram corretamente classificadas e, 
considerando-se os olhos com glaucoma, o acerto foi de 68,6$\%$.

A análise de componentes independentes foi utilizada em \cite{article:icaeels:2005} para a análise de sinais de espectometria eletrônica 
de perda de energia (EELS - \textit{Electron Energy Loss Spectroscopy}). A EELS \cite{book:EELS:1996} pode ser empregada para medições 
precisas de espessura (com resolução da ordem de 0.1~nm), pressão e análise de composição química. A ICA, através do 
algoritmo SOBI \cite{article:Cardoso:1997}, foi foi utilizada como ferramenta complementar de análise dos espectros eletrônicos 
produzidos. O uso da ICA posssibilitou a análise simultânea de dois espectros misturados e eliminou escolhas 
subjetivas durante a análise (que sem o uso da ICA precisam ser feitas pelo usuário).

O modelo não-linear da ICA também é utilizado em problemas de extração de características, por exemplo, o trabalho 
\cite{article:eegnlica:2009} ilusta a aplicação da NLICA num problema de classificação de sinais de eletroencefalograma (EEG). O objetivo é a 
separação das diferentes atividades cerebrais independentes, porém como não há garantia que o processo de combinação é 
linear, utilizou-se o modelo da NLICA numa tentativa de modelar dinâmicas cerebrais não-lineares. O modelo pós não-linear 
(PNL) foi empregado para estimar os componentes independentes. A informação mútua foi utilizada como medida da 
independência e um algoritmo genético \cite{Book:Goldberg:1989} buscou sua minimização. Múltiplos classificadores lineares (cada um 
treinado com um dos componentes estimados) foram utilizados para identificar os sinais provenientes do movimento da 
mão. Uma combinação das saídas dos múltiplos classificadores foi utilizada para produzir a decisão final. A eficiência 
de identificação a partir dos sinais medidos (sem pré-processamento) foi de 73,84$\%$, aumentando para 74,61$\%$ e 
77,95$\%$ quando utilizados, respectivamente, pré-processamento por ICA e NLICA.

A NLICA foi utilizada no trabalho \cite{article:acoesnlica:2009} visando à extração de características de séries temporais de ações para a 
previsão do índice diário de uma bolsa de valores. Para formar o vetor N-dimensional de entrada para a NLICA, foram 
utilizados a série com os valores de fechamento diário da bolsa e N-1 versões atrasadas desta série. O algoritmo MISEP
~\cite{article:misep:2004} foi 
utilizado para estimar os componentes independentes. Um modelo de regressão por vetor de suporte (\textit{Support 
Vector Regression}) foi utilizado para prever o comportamento da bolsa. Os resultados obtidos com a NLICA foram 
comparados com pré-processamento por ICA e PCA e as eficiências obtidas foram, respectivamente, 80$\%$, 75$\%$ e 79$\%$. 
Também neste exemplo, o modelo da NLICA mostrou-se mais eficiente para evidenciar as características discriminantes do 
problema em questão.

Em Física de Altas Energias (HEP - \textit{High-Energy Physics}) também são encontradas algumas aplicações de PCA e ICA 
para extração de características. Alguns desses trabalhos serão descritos brevemente na próxima seção. Para a NLICA, 
talvez por ser uma técnica ainda menos difundida (em comparação com PCA e ICA), não foram encontradas aplicações na 
área de HEP. 

\subsection{Física de Altas Energias e Áreas Correlatas}

A partir do final da década de 1990, os métodos
de aprendizado estatístico multi-variável como PCA, ICA e SOM vêm
sendo aplicados com sucesso em problemas na área de física de alta
energia. 

Um dos primeiros trabalhos neste tópico \cite{article:lang:som}, foi
publicado em 1998 e utiliza mapas auto-organizáveis (SOM) para a
classificação de eventos de raios gama em astronomia de alta
energia. Em~\cite{article:som:had:1999}, mapas SOM foram aplicados
para a separação de bósons W do ruído de fundo composto por jatos
hadrônicos. No trabalho~\cite{article:lange:som}, o ruído de fundo
gerado na aceleração do feixe de partículas foi rejeitado a partir de mapas auto-organizáveis
modificados. Redes SOM também foram utilizadas com sucesso para
análise, classificação e monitoramento de sinais do telescópio OGLE
(no Chile)~\cite{article:lucas:som} e para a identificação de
prováveis assinaturas de bósons de Higgs~\cite{article:aatos:som}.

A análise de componentes principais (PCA) é um técnica de
descorrelação e compactação bastante utilizada em diversas áreas do
conhecimento. Em física de alta energia, PCA foi aplicada para a
seleção de variáveis de entrada de um discriminador neural no
trabalho \cite{article:proriol:pca}. Em \cite{article:wolter:multi},
são apresentadas diversas aplicações em HEP onde é utilizada a PCA
para extração de características e compactação. No trabalho
\cite{article:akras:pca}, sinais ópticos de nebulosas planetárias
são processados por PCA com o objetivo de extrair informações a
respeito de suas características morfológicas. Numa outra aplicação
em astrofísica, PCA é utilizada, em conjunto com ICA, para a remoção
do ruído de fundo e de outras fontes de interferência, permitindo
melhor visualização de dados de ventos e tempestades solares
\cite{article:cadavid:pcaica}. O trabalho
\cite{article:herman:2006} utiliza a PCA, de forma segmentada, para
compactação de sinais de calorimetria do ATLAS, em seguida
classificadores neurais realizam a decisão elétron/jato, conseguindo
boa eficiência de classificação.

A análise de componentes independentes (ICA) tem aplicação mais
recente em HEP, sendo que um dos primeiros trabalhos foi publicado
em 2005 e descreve a elimi\-na\-ção de ruído na análise de sinais do
feixe de partículas do experimento BOOSTER do Fermilab
\cite{article:booster:ica}. Neste trabalho também é realizada uma
comparação com um sistema semelhante baseado em PCA e a ICA
apresenta resultados melhores. No trabalho
\cite{article:fernandez:2005}, ICA é utilizada para análise de dados
multi-variados em experimentos de física atômica e nuclear. A
aplicação de ICA proporcionou redução do ruído de fundo, permitindo
melhor visualização do sinal de interesse. ICA também foi aplicado
com sucesso para separação de sinais em astrofísica de alta energia
conforme detalhado a seguir. Em \cite{article:costagli:ica}, ICA foi
aplicada para a separação de imagens de fontes sobrepostas
adquiridas pelo satélite Planck da Agência Espacial Européia; no
trabalho \cite{article:igual:ica} utiliza-se a análise de
componentes independentes, em substituição aos filtros casados, para
a decomposição de sinais astrofísicos simulados compostos pela
combinação de moléculas elementares em estado congelado. Ainda na
área de astrofísica, nos trabalhos
\cite{article:cardoso:2005,article:vio:ica} ICA foi aplicado para a
caracterização da radiação cósmica de fundo em microondas (CMB -
\textit{Cosmic Microwave Background}). A CMB é uma forma de energia
eletromagnética que preenche todo o universo e foi inicialmente
observada em 1965. A CMB é visualizada apenas por rádio-telescópios.

A partir destes exemplos, percebe-se que, apesar da aplicação mais
recente em física de alta energia e áreas correlatas, diversos
problemas de extração de ca\-rac\-te\-rísticas, remoção de ruído,
agrupamento não-supervisionado (\textit{clustering}) e visualização vêm
sendo resolvidos com a aplicação das técnicas estatísticas de
processamento não-supervisionado de sinais.



